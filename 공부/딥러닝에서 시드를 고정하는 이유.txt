딥러닝에서 랜덤 시드 고정하는 이유


시드를 고정하는 이유
1. 재현성:
	딥러닝 모델 훈련 과정에서 시드 고정을 하지 않으면 무작위성 때문에 다시 결과를 재현하기 어렵습니다.
	시드를 고정하면 모델이 훈련될 때마다 동일한 초기값이 사용되어 결과 복제 및 비교가 쉬워짐
	
2. 모델 안정성:
	무작위성은 훈련 중에 모델 성능에 영향을 줄 수 있습니다.
	시드 고정을 통해 학습 과정을 더 효과적으로 제어할 수 있어서
	모델을 미세 조정하거나 다양한 아키텍처를 비교할 때 특히 중요한 역할을 함
	
3. 디버깅 및 문제 해결:
	예상치 못한 결과나 오류가 발생할 경우에도 시드를 고정합니다.
	이렇게 하면 디버깅 프로세르가 단순화됩니다.
	분석가와 개발자는 문제 식별 및 해결에만 집중함으로써
	문제를 보다 효과적으로 찾아낼 수 있습니다.
	
4. 하이퍼파라미터 튜닝:
	하이퍼파라미터 튜닝을 할 때 시드를 고정하는 것이 원칙입니다.
	그래야 하이퍼파라미터의 다양한 구성 세트를 공정하게 비교할 수 있습니다.
	이 방법을 사용하면 모델 성능에서 발생한 차이가 하이퍼파라미터로 인해 발생한것들만 볼 수 있습니다.
	하이퍼파라미터 최적값이 할 때 마다 너무 다른 값이 나온 경우는 시드를 고정하지 않은것인지
	의싱해 봐야 합니다.